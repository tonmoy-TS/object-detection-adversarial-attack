{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNOTATION CONVERSION\n",
    "\n",
    "def yoloconversion(file, label, imagesize):\n",
    "    dataframe = pd.read_csv(file, sep=\" \", header=None)\n",
    "    dataframe = dataframe.loc[dataframe[0] == label]\n",
    "\n",
    "    xmin = (dataframe[1] - dataframe[3]/2) * imagesize[0]\n",
    "    dataframe['xmin'] = xmin\n",
    "    xmax = (dataframe[1] + dataframe[3]/2) * imagesize[0]\n",
    "    dataframe['xmax'] = xmax\n",
    "    ymin = (dataframe[2] - dataframe[4]/2) * imagesize[1]\n",
    "    dataframe['ymin'] = ymin\n",
    "    ymax = (dataframe[2] + dataframe[4]/2) * imagesize[1]\n",
    "    dataframe['ymax'] = ymax\n",
    "    \n",
    "    if dataframe.shape[0] == 0:\n",
    "        dataframe = [[0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "    else:\n",
    "        dataframe = dataframe.to_numpy()\n",
    "        dataframe.astype(int)\n",
    "\n",
    "    return dataframe # [x, y, w, h, class, xmin, xmax, ymin, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THE MODEL\n",
    "\n",
    "def run(model, file, label, img_dim): # (model, img file, label to analize, img_dimensions[height, width])\n",
    "    s = int(math.sqrt(img_dim[0] * img_dim[1])/2)\n",
    "    results = model(file, size=s)\n",
    "    dataframe = results.pandas().xyxy[0]\n",
    "    dataframe = dataframe.loc[dataframe['class'] == label]\n",
    "\n",
    "    if dataframe.shape[0] == 0:\n",
    "        dataframe = [[img_dim[1]/2, img_dim[0]/2, img_dim[1]/2, img_dim[0]/2, 0, label, '']]\n",
    "    else:\n",
    "        dataframe = dataframe.to_numpy()\n",
    "\n",
    "    return dataframe # [xmin, ymin, xmax, ymax, confidence, class, class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERSECTION OVER UNION\n",
    "\n",
    "def intersectionoverunion(b1, b2): # [[xmin, ymin, xmax, ymax], [xmin, ymin, xmax, ymax]]\n",
    "    dx = min(b1[2], b2[2]) - max(b1[0], b2[0])\n",
    "    dy = min(b1[3], b2[3]) - max(b1[1], b2[1])\n",
    "\n",
    "    try:\n",
    "        iou = dx*dy / ((b1[2]-b1[0])*(b1[3]-b1[1]) + (b2[2]-b2[0])*(b2[3]-b2[1]) - dx*dy)\n",
    "    except ZeroDivisionError:\n",
    "        iou = 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7062001 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "YOLOv5  2021-6-13 torch 1.8.2+cpu CPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, clear_output\n",
    "import math\n",
    "import time\n",
    "\n",
    "model = torch.hub.load('', 'custom', path='traffic_signs.pt', source='local')\n",
    "model.iou = 0.6\n",
    "\n",
    "labels = [0, 1, 2, 3]\n",
    "imagesize = [1360, 800]\n",
    "\n",
    "grid = [[4, 8, 12], [1, 4, 9]] # grid and factor\n",
    "path_attack = 'C:/Users/cibar/Documents/MEGAsync/Programacion/Object detection/yolov5/traffic_signs/train/attack/' # output path\n",
    "patch_color = [(0, 0, 255), (255, 0, 0), (0, 0, 0), (0, 255, 255)]\n",
    "\n",
    "path_labels = 'traffic_signs/train/labels/' # input path\n",
    "path_images = 'traffic_signs/train/images/' # input path\n",
    "\n",
    "###################################################################################################################################\n",
    "\n",
    "labels_files = os.listdir(path_labels)\n",
    "images_files = os.listdir(path_images)\n",
    "images_files_ne = [x.split('.')[0] for x in images_files] # images files without extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('attack_output.txt', 'a')\n",
    "total = len(images_files) * len(patch_color)\n",
    "starttime = time.time()\n",
    "\n",
    "for g in range(len(patch_color)):\n",
    "    opacy = []\n",
    "    for i in range(len(images_files)): # i: image number...\n",
    "        current = g * len(images_files)+(i+1)\n",
    "        f.write('IMAGE: ' + str(images_files[i]) + ' ==> ' + str(current) +'/'+ str(total) +'\\n')\n",
    "        clear_output()\n",
    "        print('Completed: ' + str(round(100*current/total, 2)) + '%')\n",
    "        print('ETA: ' + str(round(((time.time()-starttime)/(current/total))/60,0)) + ' min')\n",
    "\n",
    "        img = cv2.imread(path_images+images_files[i]) # Image i to be analyzed\n",
    "        blank_img = cv2.imread(path_images+images_files[i]) # A background for the transparent patch\n",
    "        blank_img[:, :, :] = 0\n",
    "\n",
    "        for h in range(len(labels)): # h: label number...\n",
    "            f.write('> Label: ' + str(labels[h]) + '\\n')\n",
    "            obj_df = yoloconversion(path_labels+labels_files[i], h, imagesize) # [x, y, w, h, class, xmin, xmax, ymin, ymax]\n",
    "            run_df = run(model, path_images+images_files[i], labels[h], imagesize) # [xmin, ymin, xmax, ymax, confidence, class, class_name]\n",
    "\n",
    "            iou = [] # List of iou values for each object in the image\n",
    "            for j in range(len(obj_df)): # j: object number\n",
    "\n",
    "                if sum(obj_df[j]) == sum([0, 0, 0, 0, 0, 0, 0, 0, 0]): # If there is not an object with the laber 'h'\n",
    "                    f.write('>>> Object ' + str(j) + ': There is no label ' + str(labels[h]) + ' in the image' +'\\n')\n",
    "                \n",
    "                else:\n",
    "                    iou_bb = [] # List of iou in each object (in case the model detect several bounding boxes in one image)\n",
    "                    for k in range(len(run_df)): # k: bb number (per object)\n",
    "                        iou_bb.append(intersectionoverunion([obj_df[j][5], obj_df[j][7], obj_df[j][6], obj_df[j][8]], run_df[k]))\n",
    "                    iou = max(iou_bb)\n",
    "\n",
    "                    xs = [(obj_df[j][6]-obj_df[j][5])*2/grid[0][0], (obj_df[j][8]-obj_df[j][7])*2/grid[0][0]] # extrasize according the 1st grid [xsx, xsy]\n",
    "                    bb_xs = [int(obj_df[j][5]-xs[0]), int(obj_df[j][7]-xs[1]), int(obj_df[j][6]+xs[0]), int(obj_df[j][8]+xs[1])] # extended bb [xmin, ymin, xmax, ymax]\n",
    "                    \n",
    "                    # Limit the borders of the expanded bounding box\n",
    "                    if bb_xs[0] < 0:\n",
    "                        bb_xs[0] = 0\n",
    "                    elif bb_xs[1] < 0:\n",
    "                        bb_xs[1] = 0\n",
    "                    elif bb_xs[2] > imagesize[0]:\n",
    "                        bb_xs[2] = imagesize[0]\n",
    "                    elif bb_xs[3] > imagesize[1]:\n",
    "                        bb_xs[3] = imagesize[1]\n",
    "                    \n",
    "                    # Crop an image and create a blank image\n",
    "                    cropped_img = img[bb_xs[1]:bb_xs[3], bb_xs[0]:bb_xs[2]] # cropped image [ymin:ymax, xmin:xmax]\n",
    "                    run_df_cropped = run(model, cropped_img, labels[h], cropped_img.shape) # [xmin, ymin, xmax, ymax, confidence, class, class_name]\n",
    "                    patch_img = np.zeros((cropped_img.shape[0], cropped_img.shape[1], cropped_img.shape[2]+1)) # image height*width and 3+1 channels\n",
    "                    start_loc = [xs[0], xs[1]] # center location of the first patch\n",
    "\n",
    "                    for l in range(len(grid[0])): # l: grid division number ==> [0, 1, 2, 3]\n",
    "                        grid_size = [] # grid size [x, y]\n",
    "                        grid_size.append(xs[0]*grid[0][0]/grid[0][l]/2)\n",
    "                        grid_size.append(xs[1]*grid[0][0]/grid[0][l]/2)\n",
    "\n",
    "                        for n in range(grid[0][l]+1): # n: y coordinate of the patch\n",
    "                            for o in range(grid[0][l]+1): # o: x coordinate of the patch\n",
    "                                loc = [start_loc[0] + o*grid_size[0], start_loc[1] + n*grid_size[1]]\n",
    "                                patch_img_single = cropped_img.copy()\n",
    "\n",
    "                                minimo = [int(loc[0]-grid_size[0]), int(loc[1]-grid_size[1])]\n",
    "                                maximo = [int(loc[0]+grid_size[0]), int(loc[1]+grid_size[1])]\n",
    "\n",
    "                                cv2.rectangle(patch_img_single, (int(loc[0]-grid_size[0]), int(loc[1]-grid_size[1])), (int(loc[0]+grid_size[0]), int(loc[1]+grid_size[1])), patch_color[g], -1)\n",
    "                                run_df_patch_single = run(model, patch_img_single, labels[h], cropped_img.shape)\n",
    "                                iou_patch_single = intersectionoverunion([xs[0], xs[1], xs[0]+(obj_df[j][6]-obj_df[j][5]), xs[1]+(obj_df[j][8]-obj_df[j][7])], run_df_patch_single[0])\n",
    "\n",
    "                                if iou_patch_single < 0:\n",
    "                                    iou_dif = 0\n",
    "                                elif iou_patch_single > iou:\n",
    "                                    iou_dif = 0\n",
    "                                else:\n",
    "                                    iou_dif = iou - iou_patch_single\n",
    "\n",
    "                                # Draw over the blank image\n",
    "                                patch_img_single = np.zeros((cropped_img.shape[0], cropped_img.shape[1], cropped_img.shape[2]+1))\n",
    "                                patch_img_single[int(loc[0]-grid_size[0]):int(loc[0]+grid_size[0]), int(loc[1]-grid_size[1]):int(loc[1]+grid_size[1]), 3] = iou_dif # [xmin:xmax, xmin:ymax, channel] A patch img with a opacy of iou_difference value\n",
    "                                patch_img = patch_img + patch_img_single*grid[1][l] # Add the opacy value to the blank image, all patches\n",
    "\n",
    "####################################################################################################################################\n",
    "                    # Calculate the alpha\n",
    "                    if np.amax(patch_img) == 0: # Skip the false positives (the 0 dimension patch never affects the precision)\n",
    "                        f.write('>>> Object ' + str(j) + ': False positive' +'\\n')\n",
    "                    else:\n",
    "                        alpha = 0\n",
    "                        iou_alpha = 1\n",
    "                        combo = 0\n",
    "                        numero = 1\n",
    "                        while numero > 0:\n",
    "                            alpha = alpha + 1\n",
    "\n",
    "                            patch_img_alpha = alpha/np.amax(patch_img) * patch_img\n",
    "                            patch_img_alpha[:, :, 0] = patch_color[g][0] # set the Blue patch color\n",
    "                            patch_img_alpha[:, :, 1] = patch_color[g][1] # set the Green patch color\n",
    "                            patch_img_alpha[:, :, 2] = patch_color[g][2] # set the Red patch color\n",
    "\n",
    "                            cv2.imwrite('patch_img_alpha.png', patch_img_alpha) # Save the transparent patch\n",
    "                            patch_img_alpha = cv2.imread('patch_img_alpha.png', cv2.IMREAD_UNCHANGED) # Load the transparent patch unchanged\n",
    "\n",
    "                            img_cropped_patch = cvzone.overlayPNG(cropped_img, patch_img_alpha)\n",
    "                            run_alpha = run(model, img_cropped_patch, h, img_cropped_patch.shape)\n",
    "                            iou_alpha = intersectionoverunion([xs[0], xs[1], xs[0]+(obj_df[j][6]-obj_df[j][5]), xs[1]+(obj_df[j][8]-obj_df[j][7])], run_alpha[0])\n",
    "\n",
    "                            if iou_alpha == 0:\n",
    "                                combo = combo + 1\n",
    "                            else:\n",
    "                                combo = 0\n",
    "                            if combo > 15:\n",
    "                                numero = 0\n",
    "                            if alpha > 254:\n",
    "                                numero = 0\n",
    "\n",
    "                        f.write('>>> Object ' + str(j) + ': Ok --> ' + str(alpha) + '\\n')\n",
    "                        opacy.append(alpha)\n",
    "\n",
    "####################################################################################################################################\n",
    "#                        # Plot a IOU - alpha graph\n",
    "#                        alpha_iou = [[], []]\n",
    "#                        alpha = 0\n",
    "#                        while alpha < 256:\n",
    "#                            patch_img_alpha = alpha/np.amax(patch_img) * patch_img\n",
    "#                            patch_img_alpha[:, :, 0] = patch_color[g][0] # set the Blue patch color\n",
    "#                            patch_img_alpha[:, :, 1] = patch_color[g][1] # set the Green patch color\n",
    "#                            patch_img_alpha[:, :, 2] = patch_color[g][2] # set the Red patch color\n",
    "#\n",
    "#                            cv2.imwrite('patch_img_alpha.png', patch_img_alpha) # Save the transparent patch\n",
    "#                            patch_img_alpha = cv2.imread('patch_img_alpha.png', cv2.IMREAD_UNCHANGED) # Load the transparent patch unchanged\n",
    "#\n",
    "#                            img_cropped_patch = cvzone.overlayPNG(cropped_img, patch_img_alpha)\n",
    "#\n",
    "#                            run_alpha = run(model, img_cropped_patch, h, img_cropped_patch.shape)\n",
    "#                            iou_alpha = intersectionoverunion([xs[0], xs[1], xs[0]+(obj_df[j][6]-obj_df[j][5]), xs[1]+(obj_df[j][8]-obj_df[j][7])], run_alpha[0])\n",
    "#\n",
    "#                            alpha_iou[0].append(alpha)\n",
    "#                            alpha_iou[1].append(iou_alpha)\n",
    "#                            alpha = alpha + 1\n",
    "#\n",
    "#                        plt.plot(alpha_iou[0], alpha_iou[1])\n",
    "#                        plt.xlim(0, 255)\n",
    "#                        plt.ylim(0, 1)\n",
    "#                        plt.savefig('C:/Users/cibar/Documents/MEGAsync/Programacion/Object detection/yolov5/traffic_signs/train/attack/' + str(images_files_ne[i]) + \"_alpha_iou_\" + str(h) + \"_\" + str(j) + \"_.png\")\n",
    "#                        plt.clf()\n",
    "####################################################################################################################################\n",
    "\n",
    "                        # Crop larges partches\n",
    "                        pl = [int(obj_df[j][5] - xs[0]), int(obj_df[j][7] - xs[1])] # patch location\n",
    "                        rec = [0, 0]\n",
    "                        if pl[0] < 0:\n",
    "                            pl[0] = 0\n",
    "                            rec[0] = -pl[0]\n",
    "                        elif pl[1] < 0:\n",
    "                            pl[1] = 0\n",
    "                            rec[1] = -pl[1]\n",
    "                        patch_img = patch_img[rec[0]:, rec[1]:,] # Crop the image to avoid being out of the image\n",
    "\n",
    "                        img = cvzone.overlayPNG(img, patch_img_alpha, pl) # Overlay in the location [x, y]\n",
    "                        blank_img = cvzone.overlayPNG(blank_img, patch_img_alpha, pl)\n",
    "\n",
    "        cv2.imwrite(path_attack + str(g) + images_files[i], img) # Location + color + file\n",
    "        #cv2.imwrite(path_attack + str(g) + images_files_ne[i] + '_patch.png', blank_img) # only patch\n",
    "        f.write('########################################################')\n",
    "\n",
    "    f.write('Average alpha: ' + str(sum(opacy)/len(opacy)) +'\\n')\n",
    "    f.write('########################################################' +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "\n",
    "model = torch.hub.load('', 'custom', path='traffic_signs.pt', source='local')\n",
    "model.iou = 0.7\n",
    "imagesize = [1360, 800]\n",
    "s = int(math.sqrt(imagesize[0] * imagesize[1])/2)\n",
    "\n",
    "\n",
    "print(yoloconversion('traffic_signs/val/labels/00712.txt', 0, [1360, 800]))\n",
    "print(\"-\"*100)\n",
    "\n",
    "image = 'traffic_signs/val/images/00712.jpg'\n",
    "results = model(image, size=s)\n",
    "print(results.pandas().xyxy[0])\n",
    "print(\"-\"*100)\n",
    "\n",
    "image = 'traffic_signs/val/attack/00712.jpg'\n",
    "results = model(image, size=s)\n",
    "print(results.pandas().xyxy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get():\n",
    "    endpoint = 'https://ipinfo.io/json'\n",
    "    response = requests.get(endpoint, verify = True)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return 'Status:', response.status_code, 'Problem with the request. Exiting.'\n",
    "        exit()\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    return data['ip']\n",
    "\n",
    "#get my ip\n",
    "my_ip = get()\n",
    "\n",
    "#print my ip\n",
    "print(type(my_ip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (not a git repository), for updates see https://github.com/ultralytics/yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 512, in <module>\n",
      "    opt.data, opt.cfg, opt.hyp = check_file(opt.data), check_file(opt.cfg), check_file(opt.hyp)  # check files\n",
      "  File \"c:\\Users\\cibar\\Documents\\MEGAsync\\Programacion\\Object detection\\yolov5\\utils\\general.py\", line 219, in check_file\n",
      "    assert len(files) == 1, f\"Multiple files match '{file}', specify exact path: {files}\"  # assert unique\n",
      "AssertionError: Multiple files match 'traffic_signs.yaml', specify exact path: ['.\\\\data\\\\traffic_signs.yaml', '.\\\\traffic_signs\\\\traffic_signs.yaml']\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 32 --epochs 5 --data traffic_signs.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"EjbgFCkKPcbJLcZZSHd4\")\n",
    "project = rf.workspace().project(\"traffic-signs-kpv6r\")\n",
    "dataset = project.version(2).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --img 640 --batch 8 --epochs 5 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd127202b20e1818b91126846b6ed35e0d2312d86a55e2ea155343ae375f4653"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
